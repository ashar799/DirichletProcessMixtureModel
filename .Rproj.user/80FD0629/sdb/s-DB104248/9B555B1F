{
    "contents" : "## Testing the DPplusAFT model with Gibb's Sampling\n### Script to test if the model works with higher dimensions\n## With irrelevant features and with Time Censoring \n\nrm(list = ls())\nsetwd('/home/bit/ashar/Dropbox/Code/DPmixturemodel/DPplusAFT')\n\nlibrary(MASS)\nlibrary(mixtools)\nlibrary(matrixcalc)\nlibrary(stats)\nlibrary(Runuran)\nlibrary(truncnorm)\nlibrary(Matrix)\nlibrary(MCMCpack)\nlibrary(psych)\nlibrary(VGAM)\nlibrary(MixSim)\nlibrary(statmod)\nlibrary(flexclust)\nlibrary(survcomp)\nlibrary(mixAK)\nlibrary(mclust)\nlibrary(monomvn)\n\n\n#################################### SIMULATED DATA PROPERTIES ####################################################\n## Number of points\nN = 150\n\n## Number of Clusters\nF = 3\n\n## Distribution of the points within three clusters\n\np.dist = c(0.3,0.4,0.3)\n\n## Total Number of features D\n\nD = 50\n\n## Total Percentage of irrelevant feature\n\nprob.noise.feature = 0.90\n\n## Total Percentage of censoring\n\nprob.censoring = 0.05\n\n\n## Overlap between Cluster of molecular Data of the relevant features\n\nprob.overlap = 0.05\n\n## Percentage of Noise/Overlap in Time Data\n\nprob.noise = 0.05\n\n## Actual Number of Components and dimension  which are relevant\nrel.D = as.integer(D* (1-prob.noise.feature))\n## Actual Number of Irrelevant Componenets\nirrel.D = D - rel.D\n\n\n## Generating Data with overlap only with the relevant features\nA <- MixSim(MaxOmega = prob.overlap ,K = F, p = rel.D, int =c(-1.5,1.5), lim = 1e08)\n\n\ndata.mu = array(data = NA, dim =c(F,D))\ndata.S = array(data = NA, dim =c(F,D,D))\n\nfor( i in 1:F){\n  data.mu[i,1:rel.D] <- A$Mu[i,1:rel.D]\n  data.S[i,1:rel.D,1:rel.D] <- A$S[1:rel.D,1:rel.D,i]\n}\n\n## The relevant data is genereated first\nY.rel.list <- list(0)\nfor ( i in 1:F){\n  Y.rel.list[[i]] <- mvrnorm(n = as.integer(N * p.dist[i]), mu = data.mu[i,1:rel.D], Sigma = data.S[i,1:rel.D,1:rel.D])\n}\n\n## Scaling the Data as ONLY the scaled data will be used for generating the times\nY.rel.sc.list <- list(0)\nfor ( i in 1:F){\n  Y.rel.sc.list[[i]] <- scale(Y.rel.list[[i]], center = TRUE, scale = TRUE)\n}\n\n## Irrelevant features\nY.irrel.list <- list(0)\nfor ( i in 1:F){\n  mean <- runif(irrel.D,-1.5,1.5)\n  Y.irrel.list[[i]] <- mvrnorm(n = as.integer(N * p.dist[i]), mu = mean, Sigma = diag(x =1, nrow = irrel.D, ncol = irrel.D))\n}\n\n### Combining the data with relevant and irrelevant columns\ndata.old <- list(0) \nfor (i in 1:F){\n  data.old[[i]] <-  cbind(Y.rel.list[[i]], Y.irrel.list[[i]]) \n}\n\n############################################### MAKING Y from the clusters data #####################3\nY.old <- c(0)\nfor (i in 1:F){\n  Y.old <- rbind(Y.old, data.old[[i]])\n} \nY.old <- Y.old[-1,]\n\n#########################################################################################\nX <- Y.old\n\nrel.X <- as.matrix(X[,1:rel.D])\n\nobj.qr <- qr(X)\n\nrk <- obj.qr$rank\n\nalpha <- qr.Q(obj.qr)[,1:rel.D]\n\ngamma <- qr.Q(obj.qr)[,(1+rel.D):rk]\n\nmatT <- matrix(runif(n = rel.D*(rk -rel.D), min = -0.005, max= 0.005), nrow = rel.D, ncol = (rk -rel.D))\n\nmatP <- t(matT) %*% matT\n\nmax.eig <- eigen(matP)$values[1]\n\nmax.corr <- sqrt(max.eig)/sqrt(1 + max.eig)\n\nlinear.space <- gamma + alpha %*% matT\n\nirrel.X <- matrix(NA, nrow = N, ncol = irrel.D)\n\nfor ( i in 1: irrel.D){\n  \n  matTemp <- matrix(runif(n = (rk -rel.D), min = -1.5, max= 1.5),  nrow = (rk-rel.D), ncol =1)\n  irrel.X[,i] <- as.vector(linear.space %*% matTemp)\n  \n}\n\n## Checking if the covariance is indeed small\n\ncov.mat <- cov(rel.X,irrel.X)\n\nboxplot(cov.mat)\n\n## Building the full data matrix\n\nX.full <- cbind(rel.X, irrel.X)\n\n\nlevelplot(cov(X.full[,1:20]))\n\nY <- X.full\n\n#########################################################################################\n##########################################################################################\n##### Now WE DEAL WITH CLUSTERED DATA AND GENERATE NON RELEVANT FEATURES INDEPENENTLY ###\n\n\n## Selcting the beta co-efficients\n\n## The Co-efficients have to be obtained from uniform distribution between [-3,3]\nbeta.list <- list(0)\nhalf <- rel.D/2\nohalf <- rel.D - half\nfor ( i in 1:F){\n  beta.list[[i]] <- as.vector(rbind(runif(half, min = -3, max = -0.1), runif(ohalf, min = 0.1, max = 3)))\n}\n\n## The pure time is generated\ntime.pur.list <- list(0)\nfor ( i in 1:F){\n  time.pur.list[[i]] <- t(beta.list[[i]]) %*% t(Y.rel.sc.list[[i]])\n}\n\n## Simulating Time Data which is ONE dimensional\ntime.cluster <- MixSim(MaxOmega = prob.noise, K = F, p = 1, int =c(0,3))\n\ntime.noise.list <- list(0)\nfor ( i in 1:F){\n  time.noise.list[[i]] <- rnorm(as.integer(N * p.dist[i]), mean = time.cluster$Mu[i], sd = sqrt((time.cluster$S[i])))\n}\n\ntime.list <- list(0)\nfor ( i in 1:F){\n  time.list[[i]] <- time.pur.list[[i]] + time.noise.list[[i]]\n}\n\n\n\n## True Labels for the points\nc.true <- c(0)\nfor ( i in 1:F){\n  c.true <- rbind(as.matrix(c.true) , as.matrix(c(rep(i, as.integer(N * p.dist[i])))))  \n}\nc.true <- as.factor(c.true[-1,])\n\n\n\n\n#######################################MAKING TIME from cluster data ########################################################\ntime.real <- c(0)\nfor (i in 1:F){\n  time.real <- cbind(time.real, time.list[[i]])\n} \ntime.real <- time.real[,-1]\ntime.real <- as.vector(unlist(time.real))\n\n\n####################################### Adding CENSORING INFORMATION  ################################################################\n## Adding the censoring information to the TIME\n\ncensoring <- rbinom(n = NROW(Y), size =1, prob = 1- prob.censoring)\nright.censoring.time <- min(time.real)  \n\ntime <- time.real\n\nindex.time <- which(censoring==0)\nfor ( q in 1:length(index.time)){\n  time[index.time[q]] <- right.censoring.time\n  \n}\n\n\n## Boxplots for Vizualization of the time Data without censoring\nboxplot(time.list)\n\n\n### A Quick ManWhittney U / Kruskal test  test to check if the time's of the two cluster are significantly different\n ##wilcox.test(as.vector(time.list[[1]]), as.vector(time.list[[2]]), alternative = \"two.sided\")\n\nkruskal.test(time, c.true)\n\n## Let's See if the C-index with the correct co-efficinets are good\nc.initial <-0\nYtemp <- matrix(NA, nrow = N, ncol = D)\nnumclust <- table(factor(c.true, levels = 1:F))\nactiveclass<- which(numclust!=0)\nfor ( i in 1:length(activeclass)) {\n  \n  clust <- which(c.true == activeclass[i])\n  \n  Ytemp[clust,1:D] <- scale(Y[clust,1:D], center = TRUE, scale = TRUE)\n}\n\ncumalative.survival <- c(0)\nfor ( i in 1:N){  \n  cumalative.survival[i] <- 1 - pnorm(time.real[i], mean = time.cluster$Mu[c.true[i]] + t(beta.list[[c.true[i]]]) %*% Ytemp[i,1:rel.D], sd = sqrt(time.cluster$S[c.true[i]]))\n}\n\nrisk.survival <- 1 - cumalative.survival\n\nfor ( i in 1:F){\n  clust <- which(c.true == activeclass[i])\n  c.initial[i] <- concordance.index(x = risk.survival[clust],  surv.time = time.real[clust], surv.event= c(rep(1, as.integer(N * p.dist[i]))))$c.index \n}\n\nc.initial2 <- c(0)\nfor ( i in 1:F){\n  clust <- which(c.true == activeclass[i])\n  c.initial2[i] <- survConcordance.fit(Surv(time.real[cluster], censoring[cluster]) ~ risk.survival[clust]) \n}\n\n\n## Fitting Kaplan Meier  function\nc.initial.kp <- c(0)\ntime.predicted <- c(0)\nfor ( i in 1:N){  \n  time.predicted[i] <-  rnorm(1, mean = time.cluster$Mu[c.true[i]] + t(beta.list[[c.true[i]]]) %*% Ytemp[i,1:rel.D], sd = sqrt(time.cluster$S[c.true[i]]))\n}\n\nfor (j in 1:length(activeclass)) {\n  clust <- which(c.true ==activeclass[j])\n  ob.surv <- Surv(as.vector(time.predicted[clust]), censoring[clust])\n  km <- survfit(ob.surv~1)\n  survest <- stepfun(km$time, c(1, km$surv))\n  predicted.survival <- survest(time.predicted[clust])\n  cis[j] <- concordance.index(x = predicted.survival, surv.time = time.real[clust], surv.event= censoring[clust])$c.index \n}\n\n\n\n## Let's See if the Clusters are separate WITH AND WITHOUT THE RELEVANT FEATURES\nY.rel <- c(0)\nfor (i in 1:F){\n  Y.rel <- rbind(Y.rel, Y.rel.list[[i]])\n} \nY.rel <- Y.rel[-1,]\n\n## Just the Relevant Features Data\npc <- prcomp(Y.rel)\npc.pred <- predict(pc,newdata = Y.rel)\nplot(pc.pred[,1], pc.pred[,2], pch = 19,col = c.true)\n\n## With all the features but CORRELATED DATA\npc <- prcomp(Y.old)\npc.pred <- predict(pc,newdata = Y.old)\nplot(pc.pred[,1], pc.pred[,2], pch = 19,col = c.true)\n\n## All features but uncorrelated data\npc <- prcomp(Y)\npc.pred <- predict(pc,newdata = Y)\nplot(pc.pred[,1], pc.pred[,2], pch = 19,col = c.true)\n\n############################# PARAMETERS for GIBB's SAMPLING ######################################\n\niter = 50\niter.burnin = 20\niter.thin  =5\n\n################################# GIBBS SAMPLING  ###################################################\n\nTime <- cbind(time, censoring) \nD = NCOL(Y)\nN = NROW(Y)\nK = as.integer(N)\n\n## HYPER PRIORS\n## Hyper parameters of the DP\nshape.alpha <- 2\nrate.alpha <- 1\n## Hyperparameters for the GMM\nbeta  = D+2\nro = 0.5\n\n\nsource('rchinese.R')\nalpha  = rgamma(1, shape = shape.alpha, rate = rate.alpha )\nc <-  rchinese(N,alpha)\nf <- table(factor(c, levels = 1:max(c)))\n\n## Empirical Bayes Estimate of the Hyperparameters\nepsilon = as.vector(apply(Y,2,mean))\nW = cov(Y)\n\n\n## Initialization of the parameters for Gaussian Mixture\nmu = matrix(data = NA, nrow = K, ncol = D)\nS = array(data = NA, dim =c(K,D,D))\n\n\n#Sparsity controlling hyperparameter of the BAYESIAN LASSO MODEL\nr =1\nsi = 1.78\n\nlambda2 <- numeric(K)\ntau2 = matrix(data = NA, nrow = K, ncol = D)\nbetahat = matrix(data = NA, nrow = K, ncol = D)\nsigma2 <- rep(NA, K)\nbeta0 <- rep(NA, K)\nThat <-  numeric(N)\n\n## Fitting a linear model to the whole model\nYsc <- scale(Y[1:N,1:D], center = TRUE, scale =TRUE)\nlm.data <- lm(time ~ Ysc)\nsig2.dat <-  var(lm.data$residuals)\n\n\n## Set Some Initial Values for the Cluster Parameters\n\nsource('priordraw.R')\ndisclass <- table(factor(c, levels = 1:K))\nactiveclass <- which(disclass!=0)\nfor ( j in 1:length(activeclass)){\n  \n  priorone <- priordraw(beta, W, epsilon, ro, r, si, N, D, sig2.dat)  \n  mu[activeclass[j],] <- (priorone$mu) \n  S[activeclass[j],1:D,1:D]  <- priorone$Sigma  \n  beta0[activeclass[j]] <- priorone$beta0 \n  sigma2[activeclass[j]] <- priorone$sigma2\n  betahat[activeclass[j],1:D] <- priorone$betahat \n  lambda2[activeclass[j]] <- priorone$lambda2 \n  tau2[activeclass[j], 1:D] <- priorone$tau2\n}\n\n# The Time has to be initialized\nsource('updatetime.R')\nti <- updatetime(c, Y, Time,That, beta0, betahat, sigma2)\nThat <- ti$time\n\n\n## Initialization part for the parmaters of AFT Model with k-means and Bayesian Lasso\nsource('kmeansBlasso.R')\nkm <- kmeansBlasso(Y,That, F,K, beta, W, epsilon, ro, r, si, N, D, sig2.dat, c, mu, S, beta0, betahat, sigma2, lambda2, tau2)\nc <- km$c\nmu <- km$mu\nS <- km$S\nsigma2 <- km$sigma2\nbetahat <- km$betahat\nbeta0 <- km$beta0\nlambda2 <- km$lambda2\ntau2 <- km$tau2\n\n\n\n# Testing the  k-means estimate\nsource('predicttime.R')\ntime.predicted <- predicttime(c,Y, That,Time,beta0, betahat, sigma2)$predicttime\n\n##See How close is the predicted time with the real time\nwilcox.test(as.vector(time.predicted)[index.time], as.vector(time.real)[index.time], paired = TRUE)\n\n## Prelimnary estimates of the RAND and C-INDEX index\nsource('calcindex.R')\ncindexi <- calcindex(c,Time,time.predicted)$cindex\n\n## Adjusted Initial Rand INDEX measure\nrandindexi <- adjustedRandIndex(c.true,as.factor(c))\n\n\n\n\n## Gibb's sampling \n\nsource('posteriorchineseAFT.R')\nsource('posteriorGMMparametrs.R')\nsource('posteriortimeparameters.R')\nsource('updatetime.R')\nsource('priordraw.R')\nsource('likelihood.R')\n\n\ncognate <- NA\nparam <- NA\nparamtime <- NA\nloglike<- rep(0, iter)  \ntimeparam <- NA\ntime.predicted <- c(0)\ncindex <- c(0)\n\nprint(loglikelihood(c,Y,mu,S,alpha,That, beta0, betahat, sigma2, lambda2, tau2, K, epsilon, W, beta, ro,D, r, si, Time,N, sig2.dat) )\n\n\n#################### BURNIN PHASE ###################################################\nprint(\"BURNIN...PHASE\")\nfor (o in 1:iter.burnin) {\n  \n  \n  ################## PARAMETERS OF THE DP Mixture Model ######################################################\n  ## Updating the parameters based on the observations \n  param <- posteriorGMMparametrs(c,Y,mu,S, alpha,K, epsilon, W, beta, ro,N,D )\n  mu <- param$mean\n  S <- param$precision\n  paramtime <- posteriortimeparameters(c, That, lambda2,tau2,sigma2,beta0, betahat, Y, K, epsilon, W, beta, ro,D, r, si, Time,N, sig2.data)\n  beta0 <- paramtime$beta0\n  betahat <- paramtime$betahat\n  sigma2 <- paramtime$sigma2\n  lambda2 <- paramtime$lambda2\n  tau2 <- paramtime$tau2\n  \n  ########################## THE HYPERPARAMETERS OF THE GMM #################################  \n  #   source('posteriorhyper.R')  \n  #   #  Updating the hyper paramters\n  #     hypercognate <- posteriorhyper (c, Y, mu, S, epsilon, W, beta, ro )\n  #     epsilon <- hypercognate$epsilon\n  #     W <- hypercognate$W\n  #     W <- matrix(as.matrix(W),nrow = D, ncol =D)\n  #     ro <- hypercognate$ro\n  #     \n  ################# INDICATOR VARIABLE ##################################################################\n  ## Updating the indicator variables and the parameters\n  source('posteriorchineseAFT.R')\n  cognate <- posteriorchineseAFT(c,Y,mu,S,alpha,That, beta0, betahat, sigma2, lambda2, tau2, K, epsilon, W, beta, ro,D, r, si, Time,N, sig2.dat)\n  c <- cognate$indicator\n  mu <- cognate$mean\n  S <- cognate$precision\n  beta0 <- cognate$beta0\n  betahat <- cognate$betahat\n  sigma2 <- cognate$sigma2\n  lambda2 <- cognate$lambda2\n  tau2 <- cognate$tau2\n  \n  ########################### The Concentration Parameter #################################################################\n  \n  \n  #source('posterioralpha.R') \n  ## Updating the concentration parameter\n  # alpha <- posterioralpha(c, N, alpha, shape.alpha, rate.alpha)\n  \n  \n  ######################## The Censored Times ###########################################################\n  source('updatetime.R')\n  # Updating the Time Variable\n  ti <- NA\n  ti <- updatetime(c, Y, Time,That, beta0, betahat, sigma2)\n  That <- ti$time\n  \n  print(loglikelihood(c,Y,mu,S,alpha,That, beta0, betahat, sigma2, lambda2, tau2, K, epsilon, W, beta, ro,D, r, si, Time,N, sig2.dat) )\n  \n  print(o/iter.burnin)\n} \n\n############## GIBBS SAMPLING WITH THINNING ######################################################\n\nmu.list <- list(0)\nbeta0.list <- list(0)\nbetahat.list <- list(0) \nsigma2.list <- list(0)\nlambda2.list <- list(0)\ntau2.list <- list(0)\nc.list <- list(0)\nThat.list <- list(0)\n\nprint(\"GIBB'S SAMPLING\")\ncount = 1\nfor (o in 1:iter) {\n  \n  \n  ################## PARAMETERS OF THE DP Mixture Model ######################################################\n  ## Updating the parameters based on the observations \n  param <- posteriorGMMparametrs(c,Y,mu,S, alpha,K, epsilon, W, beta, ro,N,D )\n  mu <- param$mean\n  S <- param$precision\n  paramtime <- posteriortimeparameters(c, That, lambda2,tau2,sigma2,beta0, betahat, Y, K, epsilon, W, beta, ro,D, r, si, Time,N, sig2.data)\n  beta0 <- paramtime$beta0\n  betahat <- paramtime$betahat\n  sigma2 <- paramtime$sigma2\n  lambda2 <- paramtime$lambda2\n  tau2 <- paramtime$tau2\n  \n  ########################## THE HYPERPARAMETERS OF THE GMM #################################  \n  #   source('posteriorhyper.R')  \n  #   #  Updating the hyper paramters\n  #     hypercognate <- posteriorhyper (c, Y, mu, S, epsilon, W, beta, ro )\n  #     epsilon <- hypercognate$epsilon\n  #     W <- hypercognate$W\n  #     W <- matrix(as.matrix(W),nrow = D, ncol =D)\n  #     ro <- hypercognate$ro\n  #     \n  ################# INDICATOR VARIABLE ##################################################################\n  ## Updating the indicator variables and the parameters\n  source('posteriorchineseAFT.R')\n  cognate <- posteriorchineseAFT(c,Y,mu,S,alpha,That, beta0, betahat, sigma2, lambda2, tau2, K, epsilon, W, beta, ro,D, r, si, Time,N, sig2.dat)\n  c <- cognate$indicator\n  mu <- cognate$mean\n  S <- cognate$precision\n  beta0 <- cognate$beta0\n  betahat <- cognate$betahat\n  sigma2 <- cognate$sigma2\n  lambda2 <- cognate$lambda2\n  tau2 <- cognate$tau2\n  \n  ########################### The Concentration Parameter #################################################################\n  \n  \n  #source('posterioralpha.R') \n  ## Updating the concentration parameter\n  # alpha <- posterioralpha(c, N, alpha, shape.alpha, rate.alpha)\n  \n  \n  ######################## The Censored Times ###########################################################\n  source('updatetime.R')\n  # Updating the Time Variable\n  ti <- NA\n  ti <- updatetime(c, Y, Time,That, beta0, betahat, sigma2)\n  That <- ti$time\n  \n  \n  \n  #   ## Value of the predicted p-value\n  #   source('predicttime.R') \n  #   time.predicted <- predicttime(c, Y, That, Time, beta0, betahat, sigma2)$predicttime\n  #   #  pval <- ks.test(That, time.predicted$predicttime, alternative = \"two.sided\" )$p.value\n  #   source('calcindex.R')\n  #   cindex <- calcindex(c,Time,time.predicted)$cindex\n  #   ## Value of the Log-likelihood\n  source('likelihood.R')\n  loglike[o] <-loglikelihood(c,Y,mu,S,alpha,That, beta0, betahat, sigma2, lambda2, tau2, K, epsilon, W, beta, ro,D, r, si, Time,N, sig2.dat) \n  \n  if(o%% iter.thin == 0 ){\n    mu.list[[count]] <- mu\n    beta0.list[[count]] <- beta0\n    betahat.list[[count]] <- betahat  \n    sigma2.list[[count]] <- sigma2\n    lambda2.list[[count]] <- lambda2\n    tau2.list[[count]] <- tau2\n    c.list[[count]] <- c\n    That.list[[count]] <- That\n    count <- count +1\n  }\n  \n  \n  \n  \n  print(o/iter) \n  #   print(loglike[o])\n  #   print(cindex)\n} \n\n########## ANLAYSING THE OUTPUT #######################################################\n\ncount <- count -1\n\n## Selecting that clustering which gives the maximum RAND INDEX\nri <- 1:count\n\nindex.good <- which.max(unlist(lapply(ri,function(x) adjustedRandIndex(c.true,as.factor(c.list[[x]])))))\n\n## FINAL VALUES\nc.final <- c.list[[index.good]]\nmu.final <- mu.list[[index.good]] \nbeta0.final <- beta0.list[[index.good]]\nbetahat.final <-   betahat.list[[index.good]]\nsigma2.final <- sigma2.list[[index.good]]  \nlambda2.final <- lambda2.list[[index.good]] \nThat.final <- That.list[[index.good]]\n\n\n## FINAL VALUES\n## Adjusted Rand INDEX measure\nrandindex.final <- adjustedRandIndex(c.true,as.factor(c.final))\n\n\nsource('predicttime.R')\ntime.predicted.final <- predicttime(c.final,Y, That.final,Time, beta0.final, betahat.final, sigma2.final)$predicttime\n\n\n## Prelimnary estimates of the RAND and C-INDEX index\nsource('calcindex.R')\ncindex.final <- calcindex(c.final,Time,time.predicted.final)$cindex\n\n## Calcuating the RandIndex and the C-index\nra <- c(0)\nc1 <- c(0)\nc2 <- c(0)\n\n\nfor ( i in 1:count){\n  ra[i] <- adjustedRandIndex(c.true,as.factor(c.list[[i]]))\n  time.pr <- predicttime(c.list[[i]],Y, That.list[[i]],Time, beta0.list[[i]], betahat.list[[i]], sigma2.list[[i]])$predicttime\n  c1[i] <-  calcindex(c.list[[i]],Time,time.pr)$cindex[1]\n  c2[i] <-  calcindex(c.list[[i]],Time,time.pr)$cindex[2]\n}\n\npdf('/home/bit/ashar/Dropbox/WarsawTalk/Boxplots.pdf')\nboxplot(ra,c1,c2, names = c(\"RandInd\",\"C-Ind1\",\"C-Ind2\"), main = \"Rand and Concordance Index for Simulation\")\nleg.text <- c(\"samples = 200\", \"dims = 50\", \"cluster =2\", \"relevant.dims = 4\")\nlegend(\"topright\", leg.text)\ndev.off()\n\n\n\n\nsurv.ob <- Surv(Time[,1],Time[,2])\nsurv.fit <- survfit(surv.ob ~ c.final)\nlogrank <- survdiff(surv.ob ~ c.final)\n\n\npdf('/home/bit/ashar/Dropbox/WarsawTalk/KaplanMeier.pdf')\nplot(surv.fit, col = c(\"blue\", \"green\"))\ntitle(\"Kaplan-Meier Curves\\nfor the Simulation\")\nleg.text <- c(\"LogRank Test p-value of 7.79e-09\")\nlegend(\"topright\", leg.text)\ndev.off()\n",
    "created" : 1429544894136.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "253687036",
    "id" : "9B555B1F",
    "lastKnownWriteTime" : 1429799200,
    "path" : "~/Dropbox/Code/DPmixturemodel/DPplusAFT/GESimulationFINAL.R",
    "project_path" : "GESimulationFINAL.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}